{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exam for evaluating ML skills needed for Trantor: Exercise II\n",
    "\n",
    "### Below there are a number of examples and exercises. The goal of the exam is completing as many  of the exercises as possible. The candidates could create an auxiliary .py file and read from the notebook in order to avoid excess of text. \n",
    "### It is highly recommended to create modular code in order to reuse it for the different exercises. The capacity to create modular, self-explanatory, and clean code  that could be used accross tasks will be highly appreciated.\n",
    "### Short comments could be added to explain the choice of the ML model or algorithm, as well as references to papers where a similar solution is used for a related problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the test, you are required to, given a set of data, propose a supervised learning approach that fits the problem at hand. The specification of the problem follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"rb\") as f:  # This pickle file contains the data that can be used to predict values\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 5)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 2)\n",
      "(1002, 3)\n",
      "(1002, 3)\n",
      "(1002, 10)\n",
      "(1002, 10)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n",
      "(1002, 1)\n"
     ]
    }
   ],
   "source": [
    "print(len(data))  # We visualize the shape of the data\n",
    "for i in range(len(data)):\n",
    "    print(data[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each of the positions of this file, we can find 1002 groups of elements that belong to a same category. For example, if we check the fourth position, we will find 1002 groups of ten elements of elements related to singers/music bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Beastie Boys' 'Depeche Mode' 'Ice T' ... 'Fat Boy Slim' 'Eminem'\n",
      "  'Missy Elliot']\n",
      " ['Mana' 'Wolfredo Vargas' 'El Combo show ' ... 'nan' 'nan' 'nan']\n",
      " ['Antonio orozco ' 'Pablo alboran' 'Meléndi' ... 'nan' 'nan' 'nan']\n",
      " ...\n",
      " ['Love of lesbian' 'Niños mutantes ' 'León Benavente ' ... 'nan' 'nan'\n",
      "  'nan']\n",
      " ['Txarango' 'Els catarres' 'Sau' ... 'María Jiménez ' 'Estopa' 'nan']\n",
      " ['Rolling stones' 'Love id lesbians' 'Springsteen' ... 'nan' 'nan' 'nan']]\n",
      "(1002, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data[4])\n",
    "print(data[4].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, in the sixth position, film/saga titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Airbag' 'La Guerra de las Galaxias' 'Seven' ... 'Canta' 'nan' 'nan']\n",
      " ['Mad Max' 'Batman' 'Vengadores de Marvel' ... 'nan' 'nan' 'nan']\n",
      " ['Harry poter' 'Saw' 'La milla verde ' ... 'nan' 'nan' 'nan']\n",
      " ...\n",
      " ['El padrino' 'Indiana Jones' 'La vida es bella' ... 'nan' 'nan' 'nan']\n",
      " ['Revenge' 'La casa de papel' 'Juego de tronos' ... 'Titanic'\n",
      "  'Mamma mia' 'nan']\n",
      " ['Bethoben' 'avatar' 'Diario de noa' ... 'nan' 'nan' 'nan']]\n",
      "(1002, 10)\n"
     ]
    }
   ],
   "source": [
    "print(data[6])\n",
    "print(data[6].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in these examples, even though there is a significant amount of information in these groups, in most cases, the groups are not \"complete\". In the first group related to movie titles, for example, there are eigth titles in ten possible positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Airbag' 'La Guerra de las Galaxias' 'Seven' 'Fantasia' 'Interstellar'\n",
      " 'El Club de la Lucha' 'Regreso al Futuro' 'Canta' 'nan' 'nan']\n"
     ]
    }
   ],
   "source": [
    "print(data[6][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last group, we only find three elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bethoben' 'avatar' 'Diario de noa' 'nan' 'nan' 'nan' 'nan' 'nan' 'nan'\n",
      " 'nan']\n"
     ]
    }
   ],
   "source": [
    "print(data[6][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other instances, the proportion of missing information is much larger. As an example, in the first category (places in which clothing can be bought), which consists of groups of a single element, the number of not-'nan' elements (and therefore, groups), is very low, 14/1002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['Mercadillo '], dtype='<U19'), array(['Primark'], dtype='<U19'), array(['Donde puedo'], dtype='<U19'), array(['Mercados ambulantes'], dtype='<U19'), array(['No compro'], dtype='<U19'), array(['oulets'], dtype='<U19'), array(['MI FABRICA'], dtype='<U19'), array(['Cualquier '], dtype='<U19'), array(['Mercadillo'], dtype='<U19'), array(['Mercadillos '], dtype='<U19'), array(['Mercadillo'], dtype='<U19'), array(['Mercadillo'], dtype='<U19'), array(['Primark'], dtype='<U19'), array(['mercadillo'], dtype='<U19')]\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print([x for x in data[0] if x != ['nan']])\n",
    "print(len([x for x in data[0] if x != ['nan']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides that, we have a set of five dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"variables.pickle\", \"rb\") as f:\n",
    "    variables = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1002, 5)\n",
      "[[30. 55. 97. 85. 50.]\n",
      " [ 2. 60. 90. 85. 95.]\n",
      " [65. 96. 90. 65. 90.]\n",
      " ...\n",
      " [80. 20. 50. 15. 15.]\n",
      " [99. 10. 15.  5.  1.]\n",
      " [40. 50. 85. 65.  5.]]\n"
     ]
    }
   ],
   "source": [
    "print(variables.shape)\n",
    "print(variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, in a simmilar manner to the original data, each of the variables has 1002 recorded values. The task consisits of, given one element from each group, predict the five corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given an element (2) from a group (32) in the film title category (6), predict the five dependent values. As many models as necessary can be used, e.g., you can use one model for each of the five values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad boys\n",
      "[90. 10. 45. 25. 15.]\n"
     ]
    }
   ],
   "source": [
    "print(data[6][32][2])\n",
    "print(variables[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an easy task, as it involves written text, which is not the optimal way of presenting the data to a \"common\" model. To solve that issue, we propose you the following steps to solve the exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXERCISE 2\n",
    "\n",
    "2.1) Perform the necessary transformations of the data so that it consists of six columns. The first column would consist of each element of each group in the data, and the second coulmn would contain its corresponding variables to be predicted. Following the example two cells above, one line in the dataset would be:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bad boys', 90.0, 10.0, 45.0, 25.0, 15.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[6][32][2]] + variables[32].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the same variable values correspond to all the items in that grouping, the next line in the dataset could be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Infiltrados', 90.0, 10.0, 45.0, 25.0, 15.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[6][32][3]] + variables[32].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another line, using an item from the group with elements related to music,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sfdk', 90.0, 10.0, 45.0, 25.0, 15.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[data[4][32][0]] + variables[32].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the variables are the same, as the position (32) in which the data was found has not changed.\n",
    "\n",
    "Perform any modifications that you may find fitting to this dataset, e.g., treat 'nan's differently, or any other change. Explain and jusitfy whatever transformation you perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Transform the elements to a numeric representation using word embeddings. We recommend the gensim library, but feel free to use any other. Transform the dataset again, this time appending the values obtained from the WE to the values to be predicted. This way, the dataset will now have n+5 columns, being n the number of dimension of the WE chosen for the transformation. Note that many items will consist of multiple words, with which many WE are not compatible. To that end, figure out away of \"combining\" the multiple words of one element (e.g., the mean of the different values, or any other approach you come up with)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3 Because the representation obtained from the WE may not be optimal for a supervised learning task, we ask you to build new features. To that end, we ask you to perform the following steps:\n",
    "\n",
    "2.3.1: Choose a set of \"pivot\" values. These are vectors of the same dimension as the one of the chosen WE. The values of the pivots are arbitrary. You can choose random values, zeros, ones, twos, ..., even an item which has been transformed into its vectorized form in the previous step can be used as a pivot. You have to choose 10 pivots.\n",
    "\n",
    "2.3.2: Next, you will have to compute a distance (e.g., MSE or any other that you may find more suited to this problem) from each vectorized element, to each pivot.\n",
    "\n",
    "2.3.3: These 10 values now represent each element. Append them to the variables, and now you will have a dataset consisting of 15 columns. The first ten will contain the distances from the vectorized version of the elements to each of the topics, and the last five, the variables to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "d = 100  # assuming that the dimension of the WE is 100\n",
    "pivot0 = np.random(100)\n",
    "pivot1 = np.zeros(100)\n",
    "pivot2 = np.zeros(100)+2\n",
    "pivot3 = #vectorized form of an element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that 'vec' contains the vectorized version of an element, and that '-' is a distance\n",
    "v0 = pivot0-vec\n",
    "v1 = pivot1-vec\n",
    "# v0, v1, ... v9 will be the values representig the element vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been constructed, you are required to use a neural network-based model (preferable implemented in tensorflow or pytorch) that tries to map each set of values (10), to each varialbe to be predicted (5). Rememeber that you can use 5 different models if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, we ask you to evaluate the developed approach using tools that fit the problem that you defined. We wold also like you to extract a set of conclusions. For example, the most difficult part, the one with the largest room for improvement, or the one in which you would invest more time if you could."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not an easy problem, and you will probably not obtain good results. The goal of this task is to test your capacity of critical thinking, and justifying and defending the proposed approach. Because of this, we ask you to submit the result of your work, whether the results are positive or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
